{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to perform gridsearch in GSGP\n",
    "\n",
    "The original implementation did not have the same transformation functions as those used in the ITEA algorithm, and used MAE as a cost function. The code used in this paper was modified from it's original version.\n",
    "\n",
    "## Input format:\n",
    "\n",
    "The program has a somewhat annoying file format, and it is necessary to convert the datasets that we use in other programs to an appropriate form. For documentation:\n",
    "\n",
    "> Input files are .txt files where values ​​are separated by a TAB. The first two rows of each file represent the number of independent variables of the problem and the number of instances in the dataset. Each row represents an instance, while each column contains the values ​​of a variable. The last column contains the target values. The library comes with two example input files (training and test files).\n",
    "\n",
    "> NOTE: in test mode the test file must not have the number of instances in the dataset and also the target column must be removed.\n",
    "\n",
    "## New transformation functions\n",
    "\n",
    "The following functions were added: {id, sin, cos, tanh, sqrt |. |, log, exp}\n",
    "\n",
    "## Gridsearch\n",
    "\n",
    "Since the implementation is in C ++ but python is more practical to automate the tests, this notebook implements functions that manipulate the files related to the program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path   as path\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Current directory \n",
    "cur_folder = os.getcwd()\n",
    "\n",
    "original_datasets_folder = '../../../datasets/commaSeparated'\n",
    "adapted_datasets_folder  = '../../../datasets/tabSeparated'\n",
    "\n",
    "# compile the source code\n",
    "! cd \"{cur_folder}/\"\n",
    "! g++ GP.cc -o GP.exe\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the databases\n",
    "\n",
    "You should run the cell below only if the tabSeparated folder is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating the output folder (if not exists)\n",
    "if not os.path.exists(f'{cur_folder}/{adapted_datasets_folder}'):\n",
    "    os.makedirs(f'{cur_folder}/{adapted_datasets_folder}')\n",
    "    \n",
    "# For each dataset\n",
    "for dataset in glob.glob(f'{cur_folder}/{original_datasets_folder}/*.dat'):\n",
    "\n",
    "    # Extracting the file nale\n",
    "    file_name = dataset.replace(f'{cur_folder}/{original_datasets_folder}/', '').replace('.dat', '')\n",
    "    \n",
    "    print(f'{cur_folder}/{adapted_datasets_folder}/{file_name}.txt')\n",
    "    \n",
    "    # loading with pandas\n",
    "    df = pd.read_csv(dataset, header=None, sep=',')\n",
    "    \n",
    "    nrow, nvar = df.shape\n",
    "    nvar = nvar - 1\n",
    "    \n",
    "    # Saving the train file separated with tab\n",
    "    df.to_csv(\n",
    "        f'{cur_folder}/{adapted_datasets_folder}/{file_name}.txt',\n",
    "        sep='\\t', index=None, header=None, float_format ='%1.9f'\n",
    "    )\n",
    "    \n",
    "    # appending at the beggining of the file the size of the data set\n",
    "    with open(f'{cur_folder}/{adapted_datasets_folder}/{file_name}.txt', 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(f'{nvar}\\n{nrow}\\n' +  content[:-1])\n",
    "        \n",
    "    # Train partitions are divided in 5-folds\n",
    "    if 'train' in file_name:\n",
    "        # Repeating the procedure above\n",
    "        for i, (train_index, test_index) in enumerate(KFold(n_splits=5).split(df.iloc[:, :-1])):\n",
    "            \n",
    "            df.iloc[train_index, :].to_csv(\n",
    "                f'{cur_folder}/{adapted_datasets_folder}/{file_name}-train-{i}.txt',\n",
    "                sep='\\t', index=None, header=None, float_format ='%1.9f'\n",
    "            )\n",
    "\n",
    "            with open(f'{cur_folder}/{adapted_datasets_folder}/{file_name}-train-{i}.txt', 'r+') as f:\n",
    "                content = f.read()\n",
    "                f.seek(0, 0)\n",
    "                f.write(f'{nvar}\\n{len(train_index)}\\n' +  content[:-1])\n",
    "                \n",
    "            df.iloc[test_index, :].to_csv(\n",
    "                f'{cur_folder}/{adapted_datasets_folder}/{file_name}-validation-{i}.txt',\n",
    "                sep='\\t', index=None, header=None, float_format ='%1.9f'\n",
    "            )\n",
    "\n",
    "            with open(f'{cur_folder}/{adapted_datasets_folder}/{file_name}-validation-{i}.txt', 'r+') as f:\n",
    "                content = f.read()\n",
    "                f.seek(0, 0)\n",
    "                f.write(f'{nvar}\\n{len(test_index)}\\n' +  content[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions that perform gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Update the configuration file with a new configuration to run\n",
    "def update_config(conf, pred=False):\n",
    "    data = []\n",
    "    with open(f'{cur_folder}/configuration.ini', 'r+') as f:\n",
    "        data = f.read().splitlines()\n",
    "\n",
    "    for line_id, line in enumerate(data):\n",
    "        param = line.split(' ')[0]\n",
    "        value = line.split(' ')[-1]\n",
    "        \n",
    "        if param == 'expression_file':\n",
    "            data[line_id] = line.replace(str(value), f'{1 if pred else 0}\\n')\n",
    "        elif param == 'USE_TEST_SET': \n",
    "            data[line_id] = line.replace(str(value), f'{1 if pred else 0}\\n')\n",
    "        else:\n",
    "            data[line_id] = line.replace(str(value), f'{conf[param]}\\n')\n",
    "\n",
    "    with open(f'{cur_folder}/configuration.ini', 'w') as f:\n",
    "        f.writelines( data )\n",
    "\n",
    "        \n",
    "# function that takes a train and test dataset, the parameters to use in the\n",
    "# algorithm, and performs a single run.\n",
    "def run(dataset_train, dataset_test, conf=None):\n",
    "    update_config(conf, pred=False)\n",
    "    \n",
    "    ! ./GP.exe -train_file {dataset_train} -test_file {dataset_test}\n",
    "\n",
    "    # Getting the best fitness on train and test\n",
    "    fitness_train    = pd.read_csv(f'{cur_folder}/fitnesstrain.txt', header=None)\n",
    "    fitness_test     = pd.read_csv(f'{cur_folder}/fitnesstest.txt', header=None)\n",
    "    \n",
    "    return (\n",
    "        fitness_train.iloc[-1, 0],\n",
    "        fitness_test.iloc[-1, 0],\n",
    "    )\n",
    "\n",
    "\n",
    "# Our gridsearch implementation.\n",
    "# Function that receives training and validation partitions (5-fold), and a\n",
    "# list of settings, and determines the best setting.\n",
    "# Returns the average RMSE of the best configuration, a dictionary of the best\n",
    "# configuration, and the id of the best configuration.\n",
    "# The configuration found will be used in all repetitions of a training-test division.\n",
    "# The last 2 parameters are for saving information\n",
    "def gridsearch(dataset_train_cv, dataset_validation_cv, confs, ds, fold):\n",
    "\n",
    "    # Creating checkpoints during the gridsearch\n",
    "    gridDF = pd.DataFrame(columns = ['dataset', 'Fold', 'conf'] + [f'RMSE_{i}' for i in range(5)])\n",
    "    gridDF = gridDF.set_index(['dataset', 'Fold', 'conf'])\n",
    "\n",
    "    if os.path.isfile('../../../results/gridsearch/GSGP-gridsearch.csv'):\n",
    "        gridDF = pd.read_csv('../../../results/gridsearch/GSGP-gridsearch.csv')\n",
    "        gridDF = gridDF.set_index(['dataset', 'Fold', 'conf'])\n",
    "    \n",
    "    # (rmse_cv, configuration, configuration index)\n",
    "    best_conf = (np.inf, None, -1)\n",
    "    \n",
    "    for i, conf in enumerate(confs):\n",
    "        \n",
    "        update_config(conf, pred=False)\n",
    "        \n",
    "        if gridDF.index.isin([(ds, fold, i)]).any():\n",
    "            print(f'successfully loaded result for configuration {i}')\n",
    "        else:\n",
    "            print(f'Testing configuration {i}/{len(confs)}', end='')\n",
    "\n",
    "            gridDF.loc[(ds, fold, i), :] = (np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "            gridDF.to_csv('../../../results/gridsearch/GSGP-gridsearch.csv', index=True)\n",
    "        \n",
    "        RMSE_cv = []\n",
    "        for j, (train_cv, validation_cv) in enumerate(zip(dataset_train_cv, dataset_validation_cv)):\n",
    "            if not np.isnan(gridDF.loc[(ds, fold, i), f'RMSE_{j}']):\n",
    "                RMSE_cv.append(gridDF.loc[(ds, fold, i), f'RMSE_{j}'])\n",
    "                print(f'recovered information for fold {j}: {RMSE_cv[-1]}')\n",
    "\n",
    "                continue\n",
    "            else:\n",
    "                print(f'evauating fold {j} on cross-validation...')\n",
    "                    \n",
    "            ! ./GP.exe -train_file {train_cv} -test_file {validation_cv}\n",
    "            \n",
    "            cv_test = pd.read_csv(f'{cur_folder}/fitnesstest.txt', header=None)\n",
    "            RMSE_cv.append(cv_test.iloc[-1, 0])\n",
    "\n",
    "            # Here we know that this line exists\n",
    "            gridDF.loc[(ds, fold, i), f'RMSE_{j}'] = cv_test.iloc[-1, 0]\n",
    "            \n",
    "            gridDF.to_csv('../../../results/gridsearch/GSGP-gridsearch.csv', index=True)\n",
    "\n",
    "        if np.mean(RMSE_cv) < best_conf[0]:\n",
    "            best_conf = (np.mean(RMSE_cv), conf,  i)\n",
    "            \n",
    "    return best_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gridsearch_configurations is a dictionary, where each key is a parameter\n",
    "# and its value can be one of two options:\n",
    "# - list (python native):\n",
    "#       contains the values that will be used in the search\n",
    "# - lambda functions:\n",
    "#       used for dynamic parameters, that assumes the value based on others\n",
    "\n",
    "# Creation of parameters: a Cartesian product will be made\n",
    "# over all passed lists, then the lambda functions will be applied\n",
    "# about each configuration obtained.\n",
    "\n",
    "gridsearch_configurations = {\n",
    "    'population_size'        : [100, 250, 500],\n",
    "    'max_number_generations' : lambda conf:  100000//conf['population_size'],\n",
    "    'init_type'              : 2,\n",
    "    'p_crossover'            : [0.2, 0.5, 0.8],\n",
    "    'p_mutation'             : lambda conf: 1 - conf['p_crossover'],\n",
    "    'max_depth_creation'     : 5,\n",
    "    'tournament_size'        : 4,\n",
    "    'zero_depth'             : 0,\n",
    "    'mutation_step'          : 1.0, \n",
    "    'num_random_constants'   : 0,\n",
    "    'min_random_constant'    : -100,\n",
    "    'max_random_constant'    : 100,\n",
    "    'minimization_problem'   : 1,\n",
    "    'random_tree'            : 500\n",
    "    \n",
    "    # As de baixo ficam reservadas para o script controlar. Elas são relacionadas a\n",
    "    # executar a evolução ou usar um resultado encontrado\n",
    "    \n",
    "    #'expression_file'        : 0,\n",
    "    #'USE_TEST_SET'           : 0\n",
    "}\n",
    "\n",
    "varying = []\n",
    "\n",
    "keys, values = [], []\n",
    "for k,v in gridsearch_configurations.items():\n",
    "    if isinstance(v, list):\n",
    "        values.append(v)\n",
    "        varying.append(k)\n",
    "    elif (isinstance(v, int) or isinstance(v, float)): \n",
    "        values.append([v])\n",
    "    elif callable(v):\n",
    "        continue\n",
    "    else:\n",
    "        raise Exception('Error creating the configurations')\n",
    "    keys.append(k)\n",
    "        \n",
    "confs = [dict(zip(keys,items)) for items in product(*values)]\n",
    "\n",
    "for conf in confs:\n",
    "    for k,v in gridsearch_configurations.items():\n",
    "        if callable(v):\n",
    "            conf[k] = v(conf)\n",
    "            varying.append(k)\n",
    "\n",
    "# Saving the configuration informations\n",
    "confs_df = pd.DataFrame(confs, index=[f'conf {i}' for i in range(len(confs))]).T\n",
    "confs_df.index.names = ['Parameters']\n",
    "confs_df.to_csv('GSGP-new-gridsearch_configurations.csv')\n",
    "\n",
    "confs_df.style.apply(\n",
    "    lambda x: ['background: lightgreen' if x.name in varying else '' for i in x], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRIDSEARCH\n",
    "\n",
    "A célula abaixo usa tudo que as células anteriores criaram para rodar um gridsearch. Note que esse processo envolve várias chamadas do terminal e manipulação dos arquivos que o GSGP cria na pasta.\n",
    "\n",
    "O código é feito de forma que possa ser interrompido, mas não sei se isso pode gerar efeitos colaterais (por conta de abrir arquivos, etc). \n",
    "\n",
    "Idealmente, se souber de antemão que o código precisará ser interrompido se exceder o tempo, uma opção é rodar um dataset por vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_folds       = 5\n",
    "n_runs        = 30\n",
    "runs_per_fold = n_runs//n_folds\n",
    "\n",
    "datasets = [\n",
    "    'airfoil',\n",
    "    'concrete',\n",
    "    'energyCooling',\n",
    "    'energyHeating',\n",
    "    'Geographical',\n",
    "    'towerData',\n",
    "    'tecator',\n",
    "    'wineRed',\n",
    "    'wineWhite',\n",
    "    'yacht',\n",
    "]    \n",
    "\n",
    "columns = ['dataset','conf','Fold','Rep','RMSE_cv','RMSE_train','RMSE_test']\n",
    "\n",
    "\n",
    "fname = '../../../results/rmse/GSGP-resultsregression.csv'\n",
    "results = {c:[] for c in columns}\n",
    "\n",
    "if os.path.isfile(fname):\n",
    "    resultsDF = pd.read_csv(fname)\n",
    "    results   = resultsDF.to_dict('list')\n",
    "\n",
    "for ds in datasets:\n",
    "    print(f'Gridsearch --- data set: {ds}')\n",
    "        \n",
    "    for fold in range(n_folds):\n",
    "        dataset_train_cv      = [f'{adapted_datasets_folder}/{ds}-train-{fold}-train-{i}.txt' for i in range(5)]\n",
    "        dataset_validation_cv = [f'{adapted_datasets_folder}/{ds}-train-{fold}-validation-{i}.txt' for i in range(5)]\n",
    "        \n",
    "        dataset_train         = f'{adapted_datasets_folder}/{ds}-train-{fold}.txt'\n",
    "        dataset_test          = f'{adapted_datasets_folder}/{ds}-test-{fold}.txt'\n",
    "\n",
    "        if len(glob.glob(dataset_train))==0:\n",
    "            print(f'Dataset {dataset_train} does not exist.')\n",
    "            continue\n",
    "            \n",
    "        RMSE_cv, conf, conf_id = None, None, None\n",
    "        if os.path.isfile(fname):\n",
    "            resultsDF = pd.read_csv(fname)\n",
    "            results   = resultsDF.to_dict('list')\n",
    "\n",
    "            if len(resultsDF[\n",
    "                    (resultsDF['dataset']==ds) &\n",
    "                    (resultsDF['Fold']==fold)\n",
    "                ])>0:\n",
    "                aux_resultsDF = resultsDF[\n",
    "                    (resultsDF['dataset']==ds) &\n",
    "                    (resultsDF['Fold']==fold)\n",
    "                ]\n",
    "                conf_id = aux_resultsDF['conf'].values[0]\n",
    "                RMSE_cv = aux_resultsDF['RMSE_cv'].values[0]\n",
    "                conf = confs[conf_id]\n",
    "\n",
    "                print(f'Using previously configuration: {RMSE_cv}, {conf_id}')\n",
    "\n",
    "        if RMSE_cv == conf == conf_id == None:\n",
    "            print('Evaluating fold in gridsearch')  \n",
    "            RMSE_cv, conf, conf_id = gridsearch(dataset_train_cv, dataset_validation_cv, confs, ds, fold)\n",
    "\n",
    "        for rep in range(runs_per_fold):\n",
    "            if os.path.isfile(fname):\n",
    "                resultsDF = pd.read_csv(fname)\n",
    "                results   = resultsDF.to_dict('list')\n",
    "\n",
    "                if len(resultsDF[\n",
    "                    (resultsDF['dataset']==ds) &\n",
    "                    (resultsDF['Fold']==fold)  &\n",
    "                    (resultsDF['Rep']==rep)\n",
    "                ])==1:\n",
    "                    print(f'already evaluated {ds}-{fold}-{rep}')\n",
    "\n",
    "                    continue\n",
    "\n",
    "            print(f'evaluating config {conf_id} for {ds}-{fold}-{rep}')\n",
    "            \n",
    "            RMSE_train, RMSE_test = run(dataset_train, dataset_test, conf)\n",
    "\n",
    "            results['dataset'].append(ds)\n",
    "            results['conf'].append(conf_id)\n",
    "            results['RMSE_cv'].append(RMSE_cv)\n",
    "            results['RMSE_train'].append(RMSE_train)\n",
    "            results['RMSE_test'].append(RMSE_test)\n",
    "            results['Fold'].append(fold)\n",
    "            results['Rep'].append(rep)\n",
    "\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(fname, index=False)\n",
    "\n",
    "print('done')\n",
    "\n",
    "\n",
    "# Cleaning auxiliary files\n",
    "files = ['fitnesstest.txt', 'fitnesstrain.txt', 'trace.txt']\n",
    "for f in files:\n",
    "    os.remove(f'{cur_folder}/{f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
